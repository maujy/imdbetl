{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import random\n",
    "import requests \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import concurrent.futures\n",
    "\n",
    "WORKERNUM = 100\n",
    "RETRYTIME = 0.5\n",
    "# dprint = print\n",
    "def dprint(s):\n",
    "    True\n",
    "\n",
    "DOMAIN = \"http://www.imdb.com\"\n",
    "NAMEFILE = \"./imdb_name_list.csv\"\n",
    "NMBIOFILE = \"./output/imdb_nmbio_list.csv\"\n",
    "NMBIOFIELDS = ['nconst','overview','mini_bio','spouse',\n",
    "               'trademark','trivia','quotes','salary']\n",
    "\n",
    "tStart = time.time()\n",
    "csvfile = open(NMBIOFILE, 'w')\n",
    "writer = csv.DictWriter(csvfile, fieldnames=NMBIOFIELDS)\n",
    "writer.writeheader()\n",
    "\n",
    "nmbio_df = pd.read_csv(NAMEFILE, skipinitialspace=True, usecols=['nconst'])\n",
    "nmbio_df = nmbio_df.reindex(columns = NMBIOFIELDS)\n",
    "dprint(nmbio_df.head())\n",
    "\n",
    "# get free proxy list\n",
    "def get_proxy():\n",
    "    resp = requests.get(\"https://free-proxy-list.net/\")\n",
    "    iplist = bs(resp.text, \"lxml\").select_one(\".table-striped\").select(\"tbody tr\")\n",
    "    plist = [iplist[i].select(\"td\")[0].text + \":\" + iplist[i].select(\"td\")[1].text\n",
    "            for i in range(len(iplist))]\n",
    "    return plist\n",
    "\n",
    "# random choice proxy for crawling\n",
    "pflag=0\n",
    "def get_url_data(url):\n",
    "    global pflag\n",
    "    global proxies\n",
    "    if (pflag == 0):\n",
    "        proxies = get_proxy()\n",
    "        pflag = 10000\n",
    "    while True:\n",
    "        pflag-= 1\n",
    "        proxy = {'http':'http://' + random.choice(proxies)}\n",
    "        dprint(proxy)\n",
    "        try:\n",
    "            return requests.get(url, proxies=proxy, timeout=(1,3))\n",
    "        except:\n",
    "            time.sleep(RETRYTIME)     \n",
    "            \n",
    "# use imdb nconst to get director or cast biography\n",
    "def worker(index):\n",
    "    global nmbio_df\n",
    "    print(nmbio_df.loc[index,'nconst']+\",\")\n",
    "    url = DOMAIN + \"/name/\" + nmbio_df.loc[index,'nconst'] + \"/bio\"\n",
    "    dprint(url)\n",
    "    \n",
    "    resp = get_url_data(url)\n",
    "    dprint(resp)\n",
    "    while (resp.status_code != 200):\n",
    "        dprint(\"retry ...\")\n",
    "        resp = get_url_data(url)\n",
    "        dprint(resp)\n",
    "        \n",
    "    soup = bs(resp.text, 'html5lib')\n",
    "    try:\n",
    "        # according to the tag of jumpto list to get personal information\n",
    "        biography=soup.find('div', class_='jumpto').find_all('a')\n",
    "        if (biography is not None):\n",
    "            for bio in biography:\n",
    "                category = bio[\"href\"].replace(\"#\",\"\")\n",
    "                tag = soup.find('a', attrs={'name':category}).find_next().find_next()\n",
    "                if (tag.name == \"table\"):\n",
    "                    text = \"\"\n",
    "                    for tr in tag.find_all('tr'):\n",
    "                        k = ' '.join(tr.find_next('td').text.replace(\"\\n\",\"\").split())\n",
    "                        v = ' '.join(tr.find_next('td').find_next('td').text.replace(\"\\n\", \"\").split())\n",
    "                        text += k + \":\" +v + \",\"\n",
    "                    nmbio_df.loc[index,category] = text\n",
    "                elif (tag.name == \"div\"):\n",
    "                    text = \"\"\n",
    "                    while (tag.name == \"div\"):\n",
    "                        text += ' '.join(tag.text.replace(\"\\n\",\"\").split())\n",
    "                        tag = tag.find_next_sibling()\n",
    "                    nmbio_df.loc[index,category] = text\n",
    "            global writer             \n",
    "            writer.writerow(nmbio_df.iloc[index].to_dict())\n",
    "            mbio_df.iloc[index]=\"\"\n",
    "        return\n",
    "    except:\n",
    "        traceback.print_exc(limit=1, file=sys.stdout)\n",
    "        time.sleep(RETRYTIME)  \n",
    "\n",
    "# multithread crawler\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERNUM) as executor:\n",
    "    executor.map(worker,range(0,len(nmbio_df)))\n",
    "#     executor.map(worker,range(0,10000)) \n",
    "       \n",
    "csvfile.flush()\n",
    "csvfile.close()\n",
    "\n",
    "tEnd = time.time()\n",
    "print(\"-----------------------------\")\n",
    "print(\"Getting data num: %s\"%(len(nmbio_df)))\n",
    "print(\"Concurrent worker num: %s\"%(WORKERNUM))\n",
    "print(\"Execute time: %s\"%(tEnd-tStart))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
